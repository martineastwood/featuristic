{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5834d3-94b4-468d-803e-0b008c10ac29",
   "metadata": {},
   "source": [
    "# Tuning the Genetic Feature Synthesis\n",
    "\n",
    "There are several parameters that can be used to tune the genetic algorithms in Featuristic, which we'll explore below with the `cars` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb06d8de-d1e2-4ad0-b386-3c287589a09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.1\n"
     ]
    }
   ],
   "source": [
    "import featuristic as ft\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(8888)\n",
    "\n",
    "print(ft.__version__)\n",
    "\n",
    "X, y = ft.fetch_cars_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cad1914-1776-485b-b1a6-5dc3c78ce993",
   "metadata": {},
   "source": [
    "### Complexity of the Mathematical Expressions\n",
    "\n",
    "The `parsimony_coefficient` parameter controls the complexity of the mathematical expressions used to generate new features. When set to larger values, it penalizes larger programs more heavily, thereby encouraging the creation of smaller programs. This reduces bloat, where programs become excessively large and complex without improving their performance. By discouraging overly complex expressions, computation complexity is reduced, and the interpretability of the features is enhanced.\n",
    "\n",
    "In the example below, the `parsimony_coefficient` is set to be very small, leading to larger and more complex features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ddeb97-7128-4bc8-a265-d62bbde070cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating new features...:  58%|█████████████████████████████████████████████████▎                                   | 29/50 [00:03<00:02,  7.21it/s]\n",
      "Pruning feature space...: 100%|██████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 595.60it/s]\u001b[A\n",
      "Creating new features...:  58%|█████████████████████████████████████████████████▎                                   | 29/50 [00:03<00:02,  8.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'((abs((-(-(-((displacement / ((model_year + displacement) + weight))))) + (weight + displacement))) - -(sin(displacement))) + displacement)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth = ft.GeneticFeatureSynthesis(\n",
    "    num_features=5,\n",
    "    population_size=100,\n",
    "    max_generations=50,\n",
    "    early_termination_iters=25,\n",
    "    parsimony_coefficient=0.00001,\n",
    "    return_all_features=False,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "features = synth.fit_transform(X, y)\n",
    "\n",
    "info = synth.get_feature_info()\n",
    "\n",
    "info.head()[\"formula\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fb7ad-3787-4f42-9b93-0561e53042bd",
   "metadata": {},
   "source": [
    "And in the example, below the `parsimony_coefficient` is increased to keep the features simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6704f5e5-9d26-4be6-8ef4-5d2df3068bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating new features...:  60%|███████████████████████████████████████████████████                                  | 30/50 [00:02<00:01, 11.00it/s]\n",
      "Pruning feature space...: 100%|██████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 607.39it/s]\u001b[A\n",
      "Creating new features...:  60%|███████████████████████████████████████████████████                                  | 30/50 [00:02<00:01, 11.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'abs(-(cube(((weight + displacement) - square(model_year)))))'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth = ft.GeneticFeatureSynthesis(\n",
    "    num_features=5,\n",
    "    population_size=100,\n",
    "    max_generations=50,\n",
    "    early_termination_iters=25,\n",
    "    parsimony_coefficient=0.1,\n",
    "    return_all_features=False,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "features = synth.fit_transform(X, y)\n",
    "\n",
    "info = synth.get_feature_info()\n",
    "\n",
    "info.head()[\"formula\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb8785-3f34-4f94-97a2-edeb2218ef0b",
   "metadata": {},
   "source": [
    "### Max Generations\n",
    "\n",
    "The `max_generations` parameter refers to the maximum number of evolutions or generations the algorithm will undergo before terminating. Each generation represents a cycle of selection, crossover, and mutation operations on the population of candidate solutions.\n",
    "\n",
    "The `max_generations` parameter is crucial as it determines the duration or number of iterations for which the genetic algorithm will continue evolving potential solutions. Once the specified maximum number of generations is reached, the algorithm terminates, regardless of whether the optimal solution has been found.\n",
    "\n",
    "Setting an appropriate `max_generations` value is important to balance computational resources and the algorithm's ability to converge to a satisfactory solution. If `max_generations` is too low, the algorithm may terminate prematurely before reaching an optimal solution. Conversely, if it's too high, it may lead to unnecessary computational overhead without significant improvements in solution quality.\n",
    "\n",
    "Finding the right value for `max_generations` often requires experimentation and problem-specific considerations, such as the complexity of the optimization problem, computational resources available, and the desired level of solution quality.\n",
    "\n",
    "### Early Termination\n",
    "\n",
    "The `early_termination_iters` argument sets the number of generations after which the genetic algorithm will stop if it hasn't managed to improve the best individual in the population. This prevents unnecessary computation if the algorithm seems to have converged or stalled.\n",
    "\n",
    "For example, if `early_termination_iters` is set to 10, it means that after 10 generations (iterations) without any improvement in the best individual's fitness, the algorithm will terminate prematurely as it assumes that further iterations are unlikely to yield better results.\n",
    "\n",
    "This parameter is useful for efficiency purposes, especially when dealing with computationally expensive problems, as it helps avoid unnecessary computations once progress appears to have converged.  It works well when combined with larger `max_generations` values, as it helps to avoid prolonged computation when progress appears to have plateaued.\n",
    "\n",
    "### Population Size\n",
    "\n",
    "The `population_size` refers to the number of individuals (or candidate solutions) present in each generation of the algorithm. Each individual represents a potential solution to the optimization problem being solved.\n",
    "\n",
    "The population size is a crucial parameter as it influences the diversity and exploration capability of the algorithm. A larger population size typically allows for more diverse solutions to be explored, potentially leading to a broader exploration of the solution space. However, larger populations also require more computational resources and may lead to slower convergence if not properly managed.\n",
    "\n",
    "Conversely, a smaller population size may lead to faster convergence but could suffer from premature convergence to local optima due to limited diversity.\n",
    "\n",
    "Choosing an appropriate population size is often a balancing act between exploration (diversity) and exploitation (convergence speed). It often depends on the specific problem being solved, computational resources available, and the desired balance between exploration and exploitation. Experimentation and tuning are typically required to determine the optimal population size for a given problem.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "The `GeneticFeatureSelector` class takes a parameter called `objective_function`. This function is used by the genetic algorithm to quantify how well a particular candidate solution performs with respect to the problem being optimized. \n",
    "\n",
    "In the context of feature selection, where the aim is often to identify the most informative subset of features, the `objective_function` helps gauge how well a particular feature subset performs. By evaluating various feature combinations, the algorithm navigates towards solutions that exhibit superior performance in terms of the specified objectives.\n",
    "\n",
    "Given that feature selection typically precedes more resource-intensive tasks like model selection or hyperparameter optimization, employing a straightforward yet effective `objective_function` allows rapid identification of the most important features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
