"""
Benchmark for batch program evaluation (genetic algorithm use case).

This tests the realistic scenario where we:
1. Have a population of programs
2. Evaluate them all on the same dataset
3. Repeat for multiple generations

This is where Nim should show real speedup over Python.
"""

import time
import numpy as np
import sys
from pathlib import Path
import importlib.util

# Load featuristic_lib directly from .so file
featuristic_path = Path(__file__).parent.parent / "featuristic"
spec = importlib.util.spec_from_file_location(
    "featuristic_lib", featuristic_path / "featuristic_lib.cpython-313-darwin.so"
)
featuristic_lib = importlib.util.module_from_spec(spec)
spec.loader.exec_module(featuristic_lib)

# Import Python wrapper
synthesis_path = featuristic_path / "synthesis"
sys.path.insert(0, str(synthesis_path))
# import program_nim  # Not available, skip for now


def create_random_program(depth=3, num_features=5):
    """Create a random program tree."""
    import random

    operations = ["add", "subtract", "multiply", "negate", "square", "sin"]

    def random_node(current_depth):
        if current_depth >= depth or random.random() < 0.3:
            # Leaf node - feature
            feature_idx = random.randint(0, num_features - 1)
            return {"operation": "feature", "name": f"x{feature_idx}"}

        # Internal node - operation
        op = random.choice(operations)

        if op in ["negate", "square", "sin"]:
            return {"operation": op, "left": random_node(current_depth + 1)}
        else:
            return {
                "operation": op,
                "left": random_node(current_depth + 1),
                "right": random_node(current_depth + 1),
            }

    return random_node(0)


def evaluate_program_python(program_dict, X, feature_names):
    """Pure Python recursive evaluation (baseline)."""
    op_map = {
        "add": lambda a, b: a + b,
        "subtract": lambda a, b: a - b,
        "multiply": lambda a, b: a * b,
        "negate": lambda a: -a,
        "square": lambda a: a**2,
        "sin": np.sin,
    }

    def eval_node(node):
        operation = node.get("operation", "")

        if operation == "feature":
            feature_name = node.get("name", "")
            feature_idx = feature_names.index(feature_name)
            return X[:, feature_idx]

        elif operation in ["negate", "square", "sin"]:
            left_result = eval_node(node["left"])
            return op_map[operation](left_result)

        else:  # Binary operations
            left_result = eval_node(node["left"])
            right_result = eval_node(node["right"])
            return op_map[operation](left_result, right_result)

    return eval_node(program_dict)


def serialize_program_once(program_dict, feature_names):
    """Serialize program once and cache the result."""
    nodes = []
    feature_indices = []
    op_kinds = []
    left_children = []
    right_children = []
    constants = []

    def serialize_node(node: dict) -> int:
        operation = node.get("operation", "")

        if operation == "feature":
            feature_name = node.get("name", "")
            feature_idx = feature_names.index(feature_name)

            node_idx = len(nodes)
            feature_indices.append(feature_idx)
            op_kinds.append(14)  # opFeature
            left_children.append(-1)
            right_children.append(-1)
            constants.append(0.0)
            nodes.append(node)
            return node_idx

        else:
            left_node = node.get("left")
            right_node = node.get("right")

            left_idx = serialize_node(left_node) if left_node else -1
            right_idx = serialize_node(right_node) if right_node else -1

            op_map = {
                "add": 0,
                "subtract": 1,
                "multiply": 2,
                "negate": 5,
                "square": 10,
                "sin": 6,
            }
            op_kind = op_map.get(operation, 0)

            node_idx = len(nodes)
            feature_indices.append(-1)
            op_kinds.append(op_kind)
            left_children.append(left_idx)
            right_children.append(right_idx)
            constants.append(0.0)
            nodes.append(node)
            return node_idx

    root_idx = serialize_node(program_dict)

    return {
        "feature_indices": feature_indices,
        "op_kinds": op_kinds,
        "left_children": left_children,
        "right_children": right_children,
        "constants": constants,
    }


def benchmark_batch_evaluation():
    """Benchmark batch evaluation of multiple programs."""
    print("=" * 70)
    print("BATCH PROGRAM EVALUATION BENCHMARK (Genetic Algorithm Use Case)")
    print("=" * 70)
    print("\nSimulating population evaluation across multiple generations")
    print("This is where Nim should show real speedup")
    print()

    # Setup
    feature_names = [f"x{i}" for i in range(5)]
    X = np.random.randn(10_000, 5).astype(np.float64)  # 10k samples

    # Test configurations
    configs = [
        {
            "population_size": 50,
            "generations": 10,
            "name": "Small population (50 programs, 10 generations)",
        },
        {
            "population_size": 100,
            "generations": 20,
            "name": "Medium population (100 programs, 20 generations)",
        },
        {
            "population_size": 200,
            "generations": 50,
            "name": "Large population (200 programs, 50 generations)",
        },
    ]

    for config in configs:
        print("\n" + "=" * 70)
        print(f"{config['name']}")
        print("=" * 70)

        population_size = config["population_size"]
        generations = config["generations"]

        # Create population of random programs
        programs = [create_random_program(depth=3) for _ in range(population_size)]

        # Pre-serialize all programs for Nim (amortize the cost)
        serialized_programs = [
            serialize_program_once(prog, feature_names) for prog in programs
        ]
        X_column_major = X.T.copy()  # Pre-convert data once
        feature_ptrs = [
            int(X_column_major[i, :].ctypes.data) for i in range(X.shape[1])
        ]

        # Warm up
        for prog in programs[:5]:
            evaluate_program_python(prog, X, feature_names)

        for serialized in serialized_programs[:5]:
            featuristic_lib.evaluateProgram(
                feature_ptrs,
                serialized["feature_indices"],
                serialized["op_kinds"],
                serialized["left_children"],
                serialized["right_children"],
                serialized["constants"],
                X.shape[0],
                X.shape[1],
            )

        # Benchmark Python
        python_times = []
        start_total = time.perf_counter()
        for gen in range(generations):
            gen_start = time.perf_counter()
            for prog in programs:
                evaluate_program_python(prog, X, feature_names)
            gen_end = time.perf_counter()
            python_times.append(gen_end - gen_start)

        python_total = time.perf_counter() - start_total

        # Benchmark Nim (with pre-serialized programs and pre-converted data)
        nim_times = []
        start_total = time.perf_counter()
        for gen in range(generations):
            gen_start = time.perf_counter()
            for serialized in serialized_programs:
                featuristic_lib.evaluateProgram(
                    feature_ptrs,
                    serialized["feature_indices"],
                    serialized["op_kinds"],
                    serialized["left_children"],
                    serialized["right_children"],
                    serialized["constants"],
                    X.shape[0],
                    X.shape[1],
                )
            gen_end = time.perf_counter()
            nim_times.append(gen_end - gen_start)

        nim_total = time.perf_counter() - start_total

        speedup = python_total / nim_total

        print(f"\nPython total time: {python_total*1000:8.2f}ms")
        print(f"  Average per generation: {np.mean(python_times)*1000:8.2f}ms")
        print(f"\nNim total time:    {nim_total*1000:8.2f}ms")
        print(f"  Average per generation: {np.mean(nim_times)*1000:8.2f}ms")
        print(f"\nðŸš€ Speedup: {speedup:5.2f}x")


def benchmark_single_vs_batch():
    """Compare single evaluation vs batch evaluation."""
    print("\n" + "=" * 70)
    print("SINGLE vs BATCH EVALUATION COMPARISON")
    print("=" * 70)
    print()

    feature_names = [f"x{i}" for i in range(5)]
    X = np.random.randn(10_000, 5).astype(np.float64)
    program = create_random_program(depth=4)

    # Single evaluation (with serialization overhead each time)
    print("Single evaluation (serializing each time):")
    single_times = []
    for _ in range(100):
        start = time.perf_counter()
        result = program_nim.evaluate_program_nim(program, X, feature_names)
        end = time.perf_counter()
        single_times.append(end - start)

    print(f"  Average: {np.mean(single_times)*1000:6.3f}ms")

    # Batch evaluation (serialize once, evaluate many times)
    print("\nBatch evaluation (serialize once, evaluate 100x):")
    serialized = serialize_program_once(program, feature_names)
    X_column_major = X.T.copy()
    feature_ptrs = [int(X_column_major[i, :].ctypes.data) for i in range(X.shape[1])]

    batch_times = []
    start = time.perf_counter()
    for _ in range(100):
        featuristic_lib.evaluateProgram(
            feature_ptrs,
            serialized["feature_indices"],
            serialized["op_kinds"],
            serialized["left_children"],
            serialized["right_children"],
            serialized["constants"],
            X.shape[0],
            X.shape[1],
        )
    end = time.perf_counter()
    batch_avg = (end - start) / 100

    print(f"  Average: {batch_avg*1000:6.3f}ms")
    print(f"\n  Speedup from batching: {np.mean(single_times)/batch_avg:5.2f}x")


def benchmark_parallel_evaluation():
    """Benchmark the new evaluateProgramsParallel function."""
    print("\n" + "=" * 70)
    print("PARALLEL BATCH EVALUATION (NEW)")
    print("=" * 70)
    print("\nThis tests the new evaluateProgramsParallel function")
    print("which reduces Python-Nim boundary crossings from N to 1")
    print()

    feature_names = [f"x{i}" for i in range(5)]
    X = np.random.randn(10_000, 5).astype(np.float64)

    # Test configurations
    configs = [
        {"population_size": 50, "name": "Small population (50 programs)"},
        {"population_size": 100, "name": "Medium population (100 programs)"},
        {"population_size": 200, "name": "Large population (200 programs)"},
    ]

    for config in configs:
        print("\n" + "-" * 70)
        print(f"{config['name']}")
        print("-" * 70)

        population_size = config["population_size"]

        # Create population of random programs
        programs = [create_random_program(depth=3) for _ in range(population_size)]

        # Serialize all programs
        serialized_programs = [
            serialize_program_once(prog, feature_names) for prog in programs
        ]

        # Prepare data
        X_column_major = X.T.copy()
        feature_ptrs = [
            int(X_column_major[i, :].ctypes.data) for i in range(X.shape[1])
        ]

        # Old approach: N separate calls
        print("\nOld approach (N boundary crossings):")
        old_times = []
        for _ in range(10):
            start = time.perf_counter()
            for serialized in serialized_programs:
                featuristic_lib.evaluateProgram(
                    feature_ptrs,
                    serialized["feature_indices"],
                    serialized["op_kinds"],
                    serialized["left_children"],
                    serialized["right_children"],
                    serialized["constants"],
                    X.shape[0],
                    X.shape[1],
                )
            end = time.perf_counter()
            old_times.append(end - start)

        old_avg = np.mean(old_times)
        print(f"  Time: {old_avg*1000:8.2f}ms")
        print(f"  Boundary crossings: {population_size}")

        # New approach: 1 batch call
        print("\nNew approach (1 boundary crossing):")

        # Flatten data for batch call
        program_sizes = []
        feature_indices_flat = []
        op_kinds_flat = []
        left_children_flat = []
        right_children_flat = []
        constants_flat = []

        for serialized in serialized_programs:
            prog_size = len(serialized["feature_indices"])
            program_sizes.append(prog_size)

            feature_indices_flat.extend(serialized["feature_indices"])
            op_kinds_flat.extend(serialized["op_kinds"])
            left_children_flat.extend(serialized["left_children"])
            right_children_flat.extend(serialized["right_children"])
            constants_flat.extend(serialized["constants"])

        # Warm up
        featuristic_lib.evaluateProgramsParallel(
            feature_ptrs,
            program_sizes,
            feature_indices_flat,
            op_kinds_flat,
            left_children_flat,
            right_children_flat,
            constants_flat,
            X.shape[0],
            X.shape[1],
        )

        new_times = []
        for _ in range(10):
            start = time.perf_counter()
            results = featuristic_lib.evaluateProgramsParallel(
                feature_ptrs,
                program_sizes,
                feature_indices_flat,
                op_kinds_flat,
                left_children_flat,
                right_children_flat,
                constants_flat,
                X.shape[0],
                X.shape[1],
            )
            end = time.perf_counter()
            new_times.append(end - start)

        new_avg = np.mean(new_times)
        print(f"  Time: {new_avg*1000:8.2f}ms")
        print(f"  Boundary crossings: 1")
        print(f"\n  ðŸš€ Speedup: {old_avg/new_avg:5.2f}x")
        print(f"  Boundary crossing reduction: {population_size}x")


if __name__ == "__main__":
    benchmark_batch_evaluation()
    # benchmark_single_vs_batch()  # Skip - requires program_nim module
    benchmark_parallel_evaluation()

    print("\n" + "=" * 70)
    print("Summary:")
    print("  - Batch evaluation with pre-serialized programs shows real speedup")
    print("  - This matches the genetic algorithm use case")
    print("  - Parallel batch evaluation reduces Python-Nim boundary crossings")
    print("    from N to 1, providing additional speedup")
    print("=" * 70)
